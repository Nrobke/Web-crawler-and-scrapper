#Web Crawler and Scraper

**Overview**

This project is a web crawler and scraper designed to systematically navigate through web pages, gather data, 
and extract specific information from them. The crawler component explores the internet by following links from a set of starting URLs, 
while the scraper component extracts desired data from the web pages visited by the crawler.

#Usage

**Installation**: 
First, you need to install Python on your machine. You can download and install Python from the official Python website.
Once Python is installed, follow these steps to set up the project:

1, Clone the project repository to your local machine:
git clone  https://github.com/Nrobke/Web-crawler-and-scrapper.git

2, Navigate to the project directory:
cd your-project

3, Install all the required dependencies listed in the requirements.txt file. You can do this using pip, Python's package installer:
pip install -r requirements.txt
This will install all the necessary libraries and packages required for the web crawler and scraper to function properly.

**Output**
After you execute the program, it will prompt you to insert the seed URL and specify the depth you want to crawl. 
Once the program completes execution, the scraped web pages for each crawled link will be saved in JSON format in the same directory.
